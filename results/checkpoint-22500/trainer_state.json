{
  "best_global_step": 15000,
  "best_metric": 0.9469386465793795,
  "best_model_checkpoint": "./results\\checkpoint-15000",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 22500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 3.707029342651367,
      "learning_rate": 1.9912000000000002e-05,
      "loss": 0.7187,
      "step": 100
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 1.8002928495407104,
      "learning_rate": 1.9823111111111112e-05,
      "loss": 0.3854,
      "step": 200
    },
    {
      "epoch": 0.04,
      "grad_norm": 3.076977491378784,
      "learning_rate": 1.9734222222222223e-05,
      "loss": 0.3516,
      "step": 300
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 6.352574348449707,
      "learning_rate": 1.9645333333333333e-05,
      "loss": 0.3127,
      "step": 400
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 3.3673818111419678,
      "learning_rate": 1.9556444444444447e-05,
      "loss": 0.3251,
      "step": 500
    },
    {
      "epoch": 0.08,
      "grad_norm": 6.92848539352417,
      "learning_rate": 1.9467555555555557e-05,
      "loss": 0.2939,
      "step": 600
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 3.7854974269866943,
      "learning_rate": 1.9378666666666668e-05,
      "loss": 0.2842,
      "step": 700
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 7.179932594299316,
      "learning_rate": 1.928977777777778e-05,
      "loss": 0.2362,
      "step": 800
    },
    {
      "epoch": 0.12,
      "grad_norm": 6.6538472175598145,
      "learning_rate": 1.9200888888888892e-05,
      "loss": 0.2931,
      "step": 900
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 4.8644700050354,
      "learning_rate": 1.9112000000000003e-05,
      "loss": 0.2513,
      "step": 1000
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 6.012215614318848,
      "learning_rate": 1.9023111111111113e-05,
      "loss": 0.2465,
      "step": 1100
    },
    {
      "epoch": 0.16,
      "grad_norm": 7.089376449584961,
      "learning_rate": 1.8934222222222224e-05,
      "loss": 0.2454,
      "step": 1200
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 2.2020092010498047,
      "learning_rate": 1.8845333333333334e-05,
      "loss": 0.2348,
      "step": 1300
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 5.398913383483887,
      "learning_rate": 1.8756444444444445e-05,
      "loss": 0.2487,
      "step": 1400
    },
    {
      "epoch": 0.2,
      "grad_norm": 25.256153106689453,
      "learning_rate": 1.8667555555555555e-05,
      "loss": 0.2363,
      "step": 1500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 4.740900039672852,
      "learning_rate": 1.857866666666667e-05,
      "loss": 0.2661,
      "step": 1600
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 4.233158111572266,
      "learning_rate": 1.848977777777778e-05,
      "loss": 0.2289,
      "step": 1700
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.117760181427002,
      "learning_rate": 1.840088888888889e-05,
      "loss": 0.2431,
      "step": 1800
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 2.8276426792144775,
      "learning_rate": 1.8312e-05,
      "loss": 0.2159,
      "step": 1900
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 3.652019500732422,
      "learning_rate": 1.8223111111111114e-05,
      "loss": 0.206,
      "step": 2000
    },
    {
      "epoch": 0.28,
      "grad_norm": 3.2232861518859863,
      "learning_rate": 1.8134222222222224e-05,
      "loss": 0.229,
      "step": 2100
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 8.716526985168457,
      "learning_rate": 1.8045333333333335e-05,
      "loss": 0.2441,
      "step": 2200
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 4.236326217651367,
      "learning_rate": 1.7956444444444445e-05,
      "loss": 0.2417,
      "step": 2300
    },
    {
      "epoch": 0.32,
      "grad_norm": 6.4546051025390625,
      "learning_rate": 1.786755555555556e-05,
      "loss": 0.201,
      "step": 2400
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.40283116698265076,
      "learning_rate": 1.777866666666667e-05,
      "loss": 0.2276,
      "step": 2500
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 5.1522111892700195,
      "learning_rate": 1.768977777777778e-05,
      "loss": 0.2653,
      "step": 2600
    },
    {
      "epoch": 0.36,
      "grad_norm": 2.1229591369628906,
      "learning_rate": 1.760088888888889e-05,
      "loss": 0.2309,
      "step": 2700
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 4.796108722686768,
      "learning_rate": 1.7512e-05,
      "loss": 0.2364,
      "step": 2800
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 5.21515417098999,
      "learning_rate": 1.742311111111111e-05,
      "loss": 0.243,
      "step": 2900
    },
    {
      "epoch": 0.4,
      "grad_norm": 7.403159141540527,
      "learning_rate": 1.7334222222222222e-05,
      "loss": 0.2616,
      "step": 3000
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 3.7388346195220947,
      "learning_rate": 1.7245333333333332e-05,
      "loss": 0.1985,
      "step": 3100
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 3.7185206413269043,
      "learning_rate": 1.7156444444444446e-05,
      "loss": 0.1878,
      "step": 3200
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.571429967880249,
      "learning_rate": 1.7067555555555557e-05,
      "loss": 0.1952,
      "step": 3300
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 2.604593276977539,
      "learning_rate": 1.6978666666666667e-05,
      "loss": 0.2261,
      "step": 3400
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 12.32008171081543,
      "learning_rate": 1.6889777777777778e-05,
      "loss": 0.251,
      "step": 3500
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.9388177394866943,
      "learning_rate": 1.680088888888889e-05,
      "loss": 0.2068,
      "step": 3600
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 10.703483581542969,
      "learning_rate": 1.6712000000000002e-05,
      "loss": 0.2179,
      "step": 3700
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 8.641530990600586,
      "learning_rate": 1.6623111111111112e-05,
      "loss": 0.2138,
      "step": 3800
    },
    {
      "epoch": 0.52,
      "grad_norm": 3.7654037475585938,
      "learning_rate": 1.6534222222222223e-05,
      "loss": 0.2179,
      "step": 3900
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 10.626426696777344,
      "learning_rate": 1.6445333333333337e-05,
      "loss": 0.2228,
      "step": 4000
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 5.91404390335083,
      "learning_rate": 1.6356444444444447e-05,
      "loss": 0.2156,
      "step": 4100
    },
    {
      "epoch": 0.56,
      "grad_norm": 9.689580917358398,
      "learning_rate": 1.6267555555555558e-05,
      "loss": 0.2224,
      "step": 4200
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 3.6494879722595215,
      "learning_rate": 1.6178666666666668e-05,
      "loss": 0.2121,
      "step": 4300
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 14.900776863098145,
      "learning_rate": 1.608977777777778e-05,
      "loss": 0.2181,
      "step": 4400
    },
    {
      "epoch": 0.6,
      "grad_norm": 13.601011276245117,
      "learning_rate": 1.600088888888889e-05,
      "loss": 0.1781,
      "step": 4500
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 4.522800922393799,
      "learning_rate": 1.5912e-05,
      "loss": 0.2085,
      "step": 4600
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 2.544440746307373,
      "learning_rate": 1.582311111111111e-05,
      "loss": 0.2095,
      "step": 4700
    },
    {
      "epoch": 0.64,
      "grad_norm": 7.561097145080566,
      "learning_rate": 1.5734222222222224e-05,
      "loss": 0.2174,
      "step": 4800
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 8.889714241027832,
      "learning_rate": 1.5645333333333334e-05,
      "loss": 0.1813,
      "step": 4900
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 7.789385795593262,
      "learning_rate": 1.5556444444444445e-05,
      "loss": 0.187,
      "step": 5000
    },
    {
      "epoch": 0.68,
      "grad_norm": 3.8471248149871826,
      "learning_rate": 1.546755555555556e-05,
      "loss": 0.2058,
      "step": 5100
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.25748324394226074,
      "learning_rate": 1.537866666666667e-05,
      "loss": 0.1824,
      "step": 5200
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 2.9257893562316895,
      "learning_rate": 1.528977777777778e-05,
      "loss": 0.2057,
      "step": 5300
    },
    {
      "epoch": 0.72,
      "grad_norm": 13.852710723876953,
      "learning_rate": 1.520088888888889e-05,
      "loss": 0.1928,
      "step": 5400
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.20523874461650848,
      "learning_rate": 1.5112000000000002e-05,
      "loss": 0.2143,
      "step": 5500
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 7.350332260131836,
      "learning_rate": 1.5023111111111112e-05,
      "loss": 0.219,
      "step": 5600
    },
    {
      "epoch": 0.76,
      "grad_norm": 5.205632209777832,
      "learning_rate": 1.4934222222222223e-05,
      "loss": 0.2206,
      "step": 5700
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 1.230516791343689,
      "learning_rate": 1.4845333333333333e-05,
      "loss": 0.1689,
      "step": 5800
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 3.770902156829834,
      "learning_rate": 1.4756444444444447e-05,
      "loss": 0.1723,
      "step": 5900
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.462549686431885,
      "learning_rate": 1.4667555555555558e-05,
      "loss": 0.2192,
      "step": 6000
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 10.055871963500977,
      "learning_rate": 1.4578666666666668e-05,
      "loss": 0.2056,
      "step": 6100
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 1.1419938802719116,
      "learning_rate": 1.4489777777777779e-05,
      "loss": 0.1927,
      "step": 6200
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.505413293838501,
      "learning_rate": 1.440088888888889e-05,
      "loss": 0.2134,
      "step": 6300
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 7.1111884117126465,
      "learning_rate": 1.4312000000000001e-05,
      "loss": 0.19,
      "step": 6400
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 3.2882981300354004,
      "learning_rate": 1.4223111111111112e-05,
      "loss": 0.2012,
      "step": 6500
    },
    {
      "epoch": 0.88,
      "grad_norm": 15.35948371887207,
      "learning_rate": 1.4134222222222222e-05,
      "loss": 0.1934,
      "step": 6600
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 9.994057655334473,
      "learning_rate": 1.4045333333333336e-05,
      "loss": 0.1857,
      "step": 6700
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 5.342377185821533,
      "learning_rate": 1.3956444444444446e-05,
      "loss": 0.198,
      "step": 6800
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.39314156770706177,
      "learning_rate": 1.3867555555555557e-05,
      "loss": 0.1873,
      "step": 6900
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 5.114381313323975,
      "learning_rate": 1.3778666666666667e-05,
      "loss": 0.1634,
      "step": 7000
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.2909447252750397,
      "learning_rate": 1.368977777777778e-05,
      "loss": 0.1987,
      "step": 7100
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.612912178039551,
      "learning_rate": 1.360088888888889e-05,
      "loss": 0.1789,
      "step": 7200
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 4.32391881942749,
      "learning_rate": 1.3512e-05,
      "loss": 0.2158,
      "step": 7300
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 8.588090896606445,
      "learning_rate": 1.3423111111111111e-05,
      "loss": 0.2101,
      "step": 7400
    },
    {
      "epoch": 1.0,
      "grad_norm": 9.23489761352539,
      "learning_rate": 1.3334222222222225e-05,
      "loss": 0.1979,
      "step": 7500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.940921052631579,
      "eval_f1": 0.940907961499241,
      "eval_loss": 0.1814747452735901,
      "eval_precision": 0.9409919119802103,
      "eval_recall": 0.940921052631579,
      "eval_runtime": 7.8409,
      "eval_samples_per_second": 969.278,
      "eval_steps_per_second": 60.58,
      "step": 7500
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.7202875018119812,
      "learning_rate": 1.3245333333333335e-05,
      "loss": 0.1315,
      "step": 7600
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 7.061978340148926,
      "learning_rate": 1.3156444444444446e-05,
      "loss": 0.1365,
      "step": 7700
    },
    {
      "epoch": 1.04,
      "grad_norm": 5.126982688903809,
      "learning_rate": 1.3067555555555556e-05,
      "loss": 0.1735,
      "step": 7800
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 3.720000982284546,
      "learning_rate": 1.2978666666666668e-05,
      "loss": 0.1404,
      "step": 7900
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 4.526494026184082,
      "learning_rate": 1.2889777777777779e-05,
      "loss": 0.1543,
      "step": 8000
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.05892391502857208,
      "learning_rate": 1.2800888888888889e-05,
      "loss": 0.1427,
      "step": 8100
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 4.257622718811035,
      "learning_rate": 1.2712000000000001e-05,
      "loss": 0.1535,
      "step": 8200
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 2.601008176803589,
      "learning_rate": 1.2623111111111113e-05,
      "loss": 0.1317,
      "step": 8300
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.20324064791202545,
      "learning_rate": 1.2534222222222224e-05,
      "loss": 0.1435,
      "step": 8400
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 5.918104648590088,
      "learning_rate": 1.2445333333333334e-05,
      "loss": 0.1629,
      "step": 8500
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.10913944244384766,
      "learning_rate": 1.2356444444444446e-05,
      "loss": 0.1356,
      "step": 8600
    },
    {
      "epoch": 1.16,
      "grad_norm": 8.820611953735352,
      "learning_rate": 1.2267555555555557e-05,
      "loss": 0.1289,
      "step": 8700
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 3.205881118774414,
      "learning_rate": 1.2178666666666667e-05,
      "loss": 0.1379,
      "step": 8800
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 11.119844436645508,
      "learning_rate": 1.2089777777777778e-05,
      "loss": 0.1475,
      "step": 8900
    },
    {
      "epoch": 1.2,
      "grad_norm": 8.877847671508789,
      "learning_rate": 1.200088888888889e-05,
      "loss": 0.1386,
      "step": 9000
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 5.162806034088135,
      "learning_rate": 1.1912000000000002e-05,
      "loss": 0.1578,
      "step": 9100
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 14.096879005432129,
      "learning_rate": 1.1823111111111113e-05,
      "loss": 0.1412,
      "step": 9200
    },
    {
      "epoch": 1.24,
      "grad_norm": 7.70584774017334,
      "learning_rate": 1.1734222222222223e-05,
      "loss": 0.1392,
      "step": 9300
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 3.728332042694092,
      "learning_rate": 1.1645333333333335e-05,
      "loss": 0.1619,
      "step": 9400
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.10451211035251617,
      "learning_rate": 1.1556444444444446e-05,
      "loss": 0.1459,
      "step": 9500
    },
    {
      "epoch": 1.28,
      "grad_norm": 9.820068359375,
      "learning_rate": 1.1467555555555556e-05,
      "loss": 0.1241,
      "step": 9600
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 5.2648210525512695,
      "learning_rate": 1.1378666666666667e-05,
      "loss": 0.1388,
      "step": 9700
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 17.268651962280273,
      "learning_rate": 1.1289777777777779e-05,
      "loss": 0.1455,
      "step": 9800
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.2181942462921143,
      "learning_rate": 1.120088888888889e-05,
      "loss": 0.1373,
      "step": 9900
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.4659178555011749,
      "learning_rate": 1.1112000000000001e-05,
      "loss": 0.1423,
      "step": 10000
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.05119668319821358,
      "learning_rate": 1.1023111111111112e-05,
      "loss": 0.1297,
      "step": 10100
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 6.239706039428711,
      "learning_rate": 1.0934222222222224e-05,
      "loss": 0.1407,
      "step": 10200
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 32.683326721191406,
      "learning_rate": 1.0845333333333334e-05,
      "loss": 0.1501,
      "step": 10300
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 9.226370811462402,
      "learning_rate": 1.0756444444444445e-05,
      "loss": 0.1317,
      "step": 10400
    },
    {
      "epoch": 1.4,
      "grad_norm": 3.7707204818725586,
      "learning_rate": 1.0667555555555555e-05,
      "loss": 0.155,
      "step": 10500
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 9.393094062805176,
      "learning_rate": 1.0578666666666667e-05,
      "loss": 0.1519,
      "step": 10600
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 3.6695446968078613,
      "learning_rate": 1.0489777777777778e-05,
      "loss": 0.1382,
      "step": 10700
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.5263938903808594,
      "learning_rate": 1.040088888888889e-05,
      "loss": 0.1386,
      "step": 10800
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 2.679503917694092,
      "learning_rate": 1.0312e-05,
      "loss": 0.1746,
      "step": 10900
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 3.6764471530914307,
      "learning_rate": 1.0223111111111113e-05,
      "loss": 0.1493,
      "step": 11000
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.34106072783470154,
      "learning_rate": 1.0134222222222223e-05,
      "loss": 0.1344,
      "step": 11100
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 7.216053009033203,
      "learning_rate": 1.0045333333333334e-05,
      "loss": 0.1323,
      "step": 11200
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 2.036250352859497,
      "learning_rate": 9.956444444444446e-06,
      "loss": 0.1285,
      "step": 11300
    },
    {
      "epoch": 1.52,
      "grad_norm": 16.322429656982422,
      "learning_rate": 9.867555555555556e-06,
      "loss": 0.1105,
      "step": 11400
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 3.164604902267456,
      "learning_rate": 9.778666666666667e-06,
      "loss": 0.148,
      "step": 11500
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 2.7090091705322266,
      "learning_rate": 9.689777777777779e-06,
      "loss": 0.1246,
      "step": 11600
    },
    {
      "epoch": 1.56,
      "grad_norm": 4.717975616455078,
      "learning_rate": 9.60088888888889e-06,
      "loss": 0.1449,
      "step": 11700
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.38199761509895325,
      "learning_rate": 9.512000000000001e-06,
      "loss": 0.1457,
      "step": 11800
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 3.784890651702881,
      "learning_rate": 9.423111111111112e-06,
      "loss": 0.1419,
      "step": 11900
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.18391525745391846,
      "learning_rate": 9.334222222222224e-06,
      "loss": 0.1395,
      "step": 12000
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.1877157986164093,
      "learning_rate": 9.245333333333334e-06,
      "loss": 0.1462,
      "step": 12100
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 1.90301513671875,
      "learning_rate": 9.156444444444445e-06,
      "loss": 0.1235,
      "step": 12200
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 9.873130798339844,
      "learning_rate": 9.067555555555555e-06,
      "loss": 0.1525,
      "step": 12300
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 2.4253897666931152,
      "learning_rate": 8.978666666666668e-06,
      "loss": 0.1181,
      "step": 12400
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 12.873839378356934,
      "learning_rate": 8.889777777777778e-06,
      "loss": 0.1235,
      "step": 12500
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 7.529948711395264,
      "learning_rate": 8.80088888888889e-06,
      "loss": 0.1455,
      "step": 12600
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.13995696604251862,
      "learning_rate": 8.712e-06,
      "loss": 0.1317,
      "step": 12700
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.7063653469085693,
      "learning_rate": 8.623111111111113e-06,
      "loss": 0.1455,
      "step": 12800
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.30616313219070435,
      "learning_rate": 8.534222222222223e-06,
      "loss": 0.1488,
      "step": 12900
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.12451522052288055,
      "learning_rate": 8.445333333333334e-06,
      "loss": 0.1395,
      "step": 13000
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 9.186661720275879,
      "learning_rate": 8.356444444444444e-06,
      "loss": 0.1246,
      "step": 13100
    },
    {
      "epoch": 1.76,
      "grad_norm": 4.476674556732178,
      "learning_rate": 8.267555555555556e-06,
      "loss": 0.1424,
      "step": 13200
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 2.7322635650634766,
      "learning_rate": 8.178666666666667e-06,
      "loss": 0.1259,
      "step": 13300
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 4.451234340667725,
      "learning_rate": 8.089777777777779e-06,
      "loss": 0.1531,
      "step": 13400
    },
    {
      "epoch": 1.8,
      "grad_norm": 7.786940574645996,
      "learning_rate": 8.00088888888889e-06,
      "loss": 0.1588,
      "step": 13500
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 4.9979472160339355,
      "learning_rate": 7.912000000000001e-06,
      "loss": 0.152,
      "step": 13600
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 2.093306541442871,
      "learning_rate": 7.823111111111112e-06,
      "loss": 0.127,
      "step": 13700
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.3164539635181427,
      "learning_rate": 7.734222222222222e-06,
      "loss": 0.1254,
      "step": 13800
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 2.9184746742248535,
      "learning_rate": 7.645333333333333e-06,
      "loss": 0.137,
      "step": 13900
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.23706357181072235,
      "learning_rate": 7.556444444444445e-06,
      "loss": 0.1633,
      "step": 14000
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.1698564291000366,
      "learning_rate": 7.4675555555555555e-06,
      "loss": 0.1673,
      "step": 14100
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 4.288269996643066,
      "learning_rate": 7.378666666666668e-06,
      "loss": 0.1572,
      "step": 14200
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 1.1419333219528198,
      "learning_rate": 7.289777777777778e-06,
      "loss": 0.1321,
      "step": 14300
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.33005648851394653,
      "learning_rate": 7.200888888888889e-06,
      "loss": 0.1406,
      "step": 14400
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 7.946434020996094,
      "learning_rate": 7.1120000000000015e-06,
      "loss": 0.1456,
      "step": 14500
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 1.7766872644424438,
      "learning_rate": 7.023111111111112e-06,
      "loss": 0.126,
      "step": 14600
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.5863909125328064,
      "learning_rate": 6.934222222222223e-06,
      "loss": 0.1484,
      "step": 14700
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.3136269748210907,
      "learning_rate": 6.845333333333334e-06,
      "loss": 0.1463,
      "step": 14800
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.2586839199066162,
      "learning_rate": 6.756444444444445e-06,
      "loss": 0.1545,
      "step": 14900
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.11273028701543808,
      "learning_rate": 6.667555555555556e-06,
      "loss": 0.1167,
      "step": 15000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9468421052631579,
      "eval_f1": 0.9469386465793795,
      "eval_loss": 0.1874980628490448,
      "eval_precision": 0.9471027435922872,
      "eval_recall": 0.9468421052631579,
      "eval_runtime": 7.8667,
      "eval_samples_per_second": 966.099,
      "eval_steps_per_second": 60.381,
      "step": 15000
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 4.092611789703369,
      "learning_rate": 6.578666666666668e-06,
      "loss": 0.0872,
      "step": 15100
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 5.1205291748046875,
      "learning_rate": 6.489777777777778e-06,
      "loss": 0.0764,
      "step": 15200
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.4841204583644867,
      "learning_rate": 6.400888888888889e-06,
      "loss": 0.1058,
      "step": 15300
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.1322711855173111,
      "learning_rate": 6.312000000000001e-06,
      "loss": 0.082,
      "step": 15400
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.23115168511867523,
      "learning_rate": 6.223111111111112e-06,
      "loss": 0.1196,
      "step": 15500
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.04312524199485779,
      "learning_rate": 6.1342222222222225e-06,
      "loss": 0.0807,
      "step": 15600
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 1.7413746118545532,
      "learning_rate": 6.045333333333334e-06,
      "loss": 0.0983,
      "step": 15700
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 7.2519145011901855,
      "learning_rate": 5.956444444444445e-06,
      "loss": 0.0988,
      "step": 15800
    },
    {
      "epoch": 2.12,
      "grad_norm": 5.296530723571777,
      "learning_rate": 5.867555555555556e-06,
      "loss": 0.1073,
      "step": 15900
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.9642099738121033,
      "learning_rate": 5.778666666666667e-06,
      "loss": 0.0917,
      "step": 16000
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.24003951251506805,
      "learning_rate": 5.689777777777778e-06,
      "loss": 0.0923,
      "step": 16100
    },
    {
      "epoch": 2.16,
      "grad_norm": 7.517236232757568,
      "learning_rate": 5.6008888888888895e-06,
      "loss": 0.0854,
      "step": 16200
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 8.802153587341309,
      "learning_rate": 5.512000000000001e-06,
      "loss": 0.0858,
      "step": 16300
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.49221643805503845,
      "learning_rate": 5.423111111111111e-06,
      "loss": 0.1058,
      "step": 16400
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.544059157371521,
      "learning_rate": 5.3342222222222225e-06,
      "loss": 0.1073,
      "step": 16500
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.12276411801576614,
      "learning_rate": 5.245333333333333e-06,
      "loss": 0.1018,
      "step": 16600
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 11.637616157531738,
      "learning_rate": 5.156444444444445e-06,
      "loss": 0.1168,
      "step": 16700
    },
    {
      "epoch": 2.24,
      "grad_norm": 9.591726303100586,
      "learning_rate": 5.067555555555556e-06,
      "loss": 0.1136,
      "step": 16800
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.42195457220077515,
      "learning_rate": 4.978666666666667e-06,
      "loss": 0.1062,
      "step": 16900
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.3798307180404663,
      "learning_rate": 4.889777777777778e-06,
      "loss": 0.0583,
      "step": 17000
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.20798398554325104,
      "learning_rate": 4.8008888888888895e-06,
      "loss": 0.1006,
      "step": 17100
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.2349635660648346,
      "learning_rate": 4.712000000000001e-06,
      "loss": 0.1109,
      "step": 17200
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 5.346020221710205,
      "learning_rate": 4.623111111111111e-06,
      "loss": 0.0722,
      "step": 17300
    },
    {
      "epoch": 2.32,
      "grad_norm": 3.70754337310791,
      "learning_rate": 4.5342222222222226e-06,
      "loss": 0.0917,
      "step": 17400
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 1.3156498670578003,
      "learning_rate": 4.445333333333334e-06,
      "loss": 0.0983,
      "step": 17500
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.03951994329690933,
      "learning_rate": 4.356444444444445e-06,
      "loss": 0.0937,
      "step": 17600
    },
    {
      "epoch": 2.36,
      "grad_norm": 3.1513020992279053,
      "learning_rate": 4.267555555555556e-06,
      "loss": 0.0854,
      "step": 17700
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 24.559532165527344,
      "learning_rate": 4.178666666666667e-06,
      "loss": 0.1163,
      "step": 17800
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.3034951686859131,
      "learning_rate": 4.089777777777778e-06,
      "loss": 0.084,
      "step": 17900
    },
    {
      "epoch": 2.4,
      "grad_norm": 11.253327369689941,
      "learning_rate": 4.0008888888888895e-06,
      "loss": 0.1161,
      "step": 18000
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.620547890663147,
      "learning_rate": 3.912e-06,
      "loss": 0.091,
      "step": 18100
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 23.781431198120117,
      "learning_rate": 3.823111111111111e-06,
      "loss": 0.104,
      "step": 18200
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.04275321215391159,
      "learning_rate": 3.7342222222222226e-06,
      "loss": 0.1013,
      "step": 18300
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 1.3073099851608276,
      "learning_rate": 3.6453333333333335e-06,
      "loss": 0.099,
      "step": 18400
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 9.27822494506836,
      "learning_rate": 3.5564444444444448e-06,
      "loss": 0.102,
      "step": 18500
    },
    {
      "epoch": 2.48,
      "grad_norm": 4.322755813598633,
      "learning_rate": 3.4675555555555557e-06,
      "loss": 0.1028,
      "step": 18600
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.16194146871566772,
      "learning_rate": 3.378666666666667e-06,
      "loss": 0.1168,
      "step": 18700
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.13510967791080475,
      "learning_rate": 3.289777777777778e-06,
      "loss": 0.106,
      "step": 18800
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.8435909748077393,
      "learning_rate": 3.200888888888889e-06,
      "loss": 0.0907,
      "step": 18900
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 7.010446548461914,
      "learning_rate": 3.112e-06,
      "loss": 0.0882,
      "step": 19000
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 15.323787689208984,
      "learning_rate": 3.023111111111111e-06,
      "loss": 0.0774,
      "step": 19100
    },
    {
      "epoch": 2.56,
      "grad_norm": 1.2235819101333618,
      "learning_rate": 2.934222222222222e-06,
      "loss": 0.0917,
      "step": 19200
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.0892978310585022,
      "learning_rate": 2.845333333333334e-06,
      "loss": 0.0842,
      "step": 19300
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 3.662632703781128,
      "learning_rate": 2.756444444444445e-06,
      "loss": 0.0849,
      "step": 19400
    },
    {
      "epoch": 2.6,
      "grad_norm": 10.870308876037598,
      "learning_rate": 2.667555555555556e-06,
      "loss": 0.1279,
      "step": 19500
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.0502585805952549,
      "learning_rate": 2.578666666666667e-06,
      "loss": 0.1067,
      "step": 19600
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.33351781964302063,
      "learning_rate": 2.489777777777778e-06,
      "loss": 0.1138,
      "step": 19700
    },
    {
      "epoch": 2.64,
      "grad_norm": 3.7626242637634277,
      "learning_rate": 2.400888888888889e-06,
      "loss": 0.1137,
      "step": 19800
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 4.764153480529785,
      "learning_rate": 2.312e-06,
      "loss": 0.0737,
      "step": 19900
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 11.11931037902832,
      "learning_rate": 2.2231111111111114e-06,
      "loss": 0.094,
      "step": 20000
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.22111964225769043,
      "learning_rate": 2.1342222222222227e-06,
      "loss": 0.0996,
      "step": 20100
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 12.872552871704102,
      "learning_rate": 2.0453333333333335e-06,
      "loss": 0.1063,
      "step": 20200
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.30208539962768555,
      "learning_rate": 1.956444444444445e-06,
      "loss": 0.0981,
      "step": 20300
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 1.3366590738296509,
      "learning_rate": 1.8675555555555557e-06,
      "loss": 0.1097,
      "step": 20400
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.5580303072929382,
      "learning_rate": 1.7786666666666668e-06,
      "loss": 0.0867,
      "step": 20500
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.4189762771129608,
      "learning_rate": 1.689777777777778e-06,
      "loss": 0.0816,
      "step": 20600
    },
    {
      "epoch": 2.76,
      "grad_norm": 2.436129093170166,
      "learning_rate": 1.600888888888889e-06,
      "loss": 0.0957,
      "step": 20700
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 4.1415696144104,
      "learning_rate": 1.512e-06,
      "loss": 0.088,
      "step": 20800
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 4.443831443786621,
      "learning_rate": 1.4231111111111112e-06,
      "loss": 0.0601,
      "step": 20900
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.08656422793865204,
      "learning_rate": 1.3342222222222223e-06,
      "loss": 0.1013,
      "step": 21000
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 11.62410831451416,
      "learning_rate": 1.2453333333333334e-06,
      "loss": 0.1023,
      "step": 21100
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 3.4558091163635254,
      "learning_rate": 1.1564444444444445e-06,
      "loss": 0.0968,
      "step": 21200
    },
    {
      "epoch": 2.84,
      "grad_norm": 6.178396701812744,
      "learning_rate": 1.0675555555555558e-06,
      "loss": 0.073,
      "step": 21300
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 3.3402931690216064,
      "learning_rate": 9.786666666666669e-07,
      "loss": 0.0904,
      "step": 21400
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 4.096175193786621,
      "learning_rate": 8.897777777777778e-07,
      "loss": 0.0975,
      "step": 21500
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.08904588967561722,
      "learning_rate": 8.008888888888889e-07,
      "loss": 0.0859,
      "step": 21600
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.12332996726036072,
      "learning_rate": 7.12e-07,
      "loss": 0.1087,
      "step": 21700
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.871031641960144,
      "learning_rate": 6.231111111111111e-07,
      "loss": 0.0783,
      "step": 21800
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.29348427057266235,
      "learning_rate": 5.342222222222223e-07,
      "loss": 0.0911,
      "step": 21900
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 1.2607080936431885,
      "learning_rate": 4.453333333333334e-07,
      "loss": 0.0993,
      "step": 22000
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 1.248335361480713,
      "learning_rate": 3.5644444444444444e-07,
      "loss": 0.0956,
      "step": 22100
    },
    {
      "epoch": 2.96,
      "grad_norm": 1.176027536392212,
      "learning_rate": 2.675555555555556e-07,
      "loss": 0.0633,
      "step": 22200
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 11.658548355102539,
      "learning_rate": 1.7866666666666668e-07,
      "loss": 0.0694,
      "step": 22300
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 2.903709650039673,
      "learning_rate": 8.977777777777778e-08,
      "loss": 0.118,
      "step": 22400
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.13047343492507935,
      "learning_rate": 8.88888888888889e-10,
      "loss": 0.0932,
      "step": 22500
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.9442105263157895,
      "eval_f1": 0.9442573130150929,
      "eval_loss": 0.22229699790477753,
      "eval_precision": 0.9444443875504058,
      "eval_recall": 0.9442105263157895,
      "eval_runtime": 7.9021,
      "eval_samples_per_second": 961.769,
      "eval_steps_per_second": 60.111,
      "step": 22500
    }
  ],
  "logging_steps": 100,
  "max_steps": 22500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9056469134955648.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
