{
  "best_global_step": 7500,
  "best_metric": 0.940907961499241,
  "best_model_checkpoint": "./results\\checkpoint-7500",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 7500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 3.707029342651367,
      "learning_rate": 1.9912000000000002e-05,
      "loss": 0.7187,
      "step": 100
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 1.8002928495407104,
      "learning_rate": 1.9823111111111112e-05,
      "loss": 0.3854,
      "step": 200
    },
    {
      "epoch": 0.04,
      "grad_norm": 3.076977491378784,
      "learning_rate": 1.9734222222222223e-05,
      "loss": 0.3516,
      "step": 300
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 6.352574348449707,
      "learning_rate": 1.9645333333333333e-05,
      "loss": 0.3127,
      "step": 400
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 3.3673818111419678,
      "learning_rate": 1.9556444444444447e-05,
      "loss": 0.3251,
      "step": 500
    },
    {
      "epoch": 0.08,
      "grad_norm": 6.92848539352417,
      "learning_rate": 1.9467555555555557e-05,
      "loss": 0.2939,
      "step": 600
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 3.7854974269866943,
      "learning_rate": 1.9378666666666668e-05,
      "loss": 0.2842,
      "step": 700
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 7.179932594299316,
      "learning_rate": 1.928977777777778e-05,
      "loss": 0.2362,
      "step": 800
    },
    {
      "epoch": 0.12,
      "grad_norm": 6.6538472175598145,
      "learning_rate": 1.9200888888888892e-05,
      "loss": 0.2931,
      "step": 900
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 4.8644700050354,
      "learning_rate": 1.9112000000000003e-05,
      "loss": 0.2513,
      "step": 1000
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 6.012215614318848,
      "learning_rate": 1.9023111111111113e-05,
      "loss": 0.2465,
      "step": 1100
    },
    {
      "epoch": 0.16,
      "grad_norm": 7.089376449584961,
      "learning_rate": 1.8934222222222224e-05,
      "loss": 0.2454,
      "step": 1200
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 2.2020092010498047,
      "learning_rate": 1.8845333333333334e-05,
      "loss": 0.2348,
      "step": 1300
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 5.398913383483887,
      "learning_rate": 1.8756444444444445e-05,
      "loss": 0.2487,
      "step": 1400
    },
    {
      "epoch": 0.2,
      "grad_norm": 25.256153106689453,
      "learning_rate": 1.8667555555555555e-05,
      "loss": 0.2363,
      "step": 1500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 4.740900039672852,
      "learning_rate": 1.857866666666667e-05,
      "loss": 0.2661,
      "step": 1600
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 4.233158111572266,
      "learning_rate": 1.848977777777778e-05,
      "loss": 0.2289,
      "step": 1700
    },
    {
      "epoch": 0.24,
      "grad_norm": 2.117760181427002,
      "learning_rate": 1.840088888888889e-05,
      "loss": 0.2431,
      "step": 1800
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 2.8276426792144775,
      "learning_rate": 1.8312e-05,
      "loss": 0.2159,
      "step": 1900
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 3.652019500732422,
      "learning_rate": 1.8223111111111114e-05,
      "loss": 0.206,
      "step": 2000
    },
    {
      "epoch": 0.28,
      "grad_norm": 3.2232861518859863,
      "learning_rate": 1.8134222222222224e-05,
      "loss": 0.229,
      "step": 2100
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 8.716526985168457,
      "learning_rate": 1.8045333333333335e-05,
      "loss": 0.2441,
      "step": 2200
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 4.236326217651367,
      "learning_rate": 1.7956444444444445e-05,
      "loss": 0.2417,
      "step": 2300
    },
    {
      "epoch": 0.32,
      "grad_norm": 6.4546051025390625,
      "learning_rate": 1.786755555555556e-05,
      "loss": 0.201,
      "step": 2400
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.40283116698265076,
      "learning_rate": 1.777866666666667e-05,
      "loss": 0.2276,
      "step": 2500
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 5.1522111892700195,
      "learning_rate": 1.768977777777778e-05,
      "loss": 0.2653,
      "step": 2600
    },
    {
      "epoch": 0.36,
      "grad_norm": 2.1229591369628906,
      "learning_rate": 1.760088888888889e-05,
      "loss": 0.2309,
      "step": 2700
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 4.796108722686768,
      "learning_rate": 1.7512e-05,
      "loss": 0.2364,
      "step": 2800
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 5.21515417098999,
      "learning_rate": 1.742311111111111e-05,
      "loss": 0.243,
      "step": 2900
    },
    {
      "epoch": 0.4,
      "grad_norm": 7.403159141540527,
      "learning_rate": 1.7334222222222222e-05,
      "loss": 0.2616,
      "step": 3000
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 3.7388346195220947,
      "learning_rate": 1.7245333333333332e-05,
      "loss": 0.1985,
      "step": 3100
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 3.7185206413269043,
      "learning_rate": 1.7156444444444446e-05,
      "loss": 0.1878,
      "step": 3200
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.571429967880249,
      "learning_rate": 1.7067555555555557e-05,
      "loss": 0.1952,
      "step": 3300
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 2.604593276977539,
      "learning_rate": 1.6978666666666667e-05,
      "loss": 0.2261,
      "step": 3400
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 12.32008171081543,
      "learning_rate": 1.6889777777777778e-05,
      "loss": 0.251,
      "step": 3500
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.9388177394866943,
      "learning_rate": 1.680088888888889e-05,
      "loss": 0.2068,
      "step": 3600
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 10.703483581542969,
      "learning_rate": 1.6712000000000002e-05,
      "loss": 0.2179,
      "step": 3700
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 8.641530990600586,
      "learning_rate": 1.6623111111111112e-05,
      "loss": 0.2138,
      "step": 3800
    },
    {
      "epoch": 0.52,
      "grad_norm": 3.7654037475585938,
      "learning_rate": 1.6534222222222223e-05,
      "loss": 0.2179,
      "step": 3900
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 10.626426696777344,
      "learning_rate": 1.6445333333333337e-05,
      "loss": 0.2228,
      "step": 4000
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 5.91404390335083,
      "learning_rate": 1.6356444444444447e-05,
      "loss": 0.2156,
      "step": 4100
    },
    {
      "epoch": 0.56,
      "grad_norm": 9.689580917358398,
      "learning_rate": 1.6267555555555558e-05,
      "loss": 0.2224,
      "step": 4200
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 3.6494879722595215,
      "learning_rate": 1.6178666666666668e-05,
      "loss": 0.2121,
      "step": 4300
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 14.900776863098145,
      "learning_rate": 1.608977777777778e-05,
      "loss": 0.2181,
      "step": 4400
    },
    {
      "epoch": 0.6,
      "grad_norm": 13.601011276245117,
      "learning_rate": 1.600088888888889e-05,
      "loss": 0.1781,
      "step": 4500
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 4.522800922393799,
      "learning_rate": 1.5912e-05,
      "loss": 0.2085,
      "step": 4600
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 2.544440746307373,
      "learning_rate": 1.582311111111111e-05,
      "loss": 0.2095,
      "step": 4700
    },
    {
      "epoch": 0.64,
      "grad_norm": 7.561097145080566,
      "learning_rate": 1.5734222222222224e-05,
      "loss": 0.2174,
      "step": 4800
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 8.889714241027832,
      "learning_rate": 1.5645333333333334e-05,
      "loss": 0.1813,
      "step": 4900
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 7.789385795593262,
      "learning_rate": 1.5556444444444445e-05,
      "loss": 0.187,
      "step": 5000
    },
    {
      "epoch": 0.68,
      "grad_norm": 3.8471248149871826,
      "learning_rate": 1.546755555555556e-05,
      "loss": 0.2058,
      "step": 5100
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.25748324394226074,
      "learning_rate": 1.537866666666667e-05,
      "loss": 0.1824,
      "step": 5200
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 2.9257893562316895,
      "learning_rate": 1.528977777777778e-05,
      "loss": 0.2057,
      "step": 5300
    },
    {
      "epoch": 0.72,
      "grad_norm": 13.852710723876953,
      "learning_rate": 1.520088888888889e-05,
      "loss": 0.1928,
      "step": 5400
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.20523874461650848,
      "learning_rate": 1.5112000000000002e-05,
      "loss": 0.2143,
      "step": 5500
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 7.350332260131836,
      "learning_rate": 1.5023111111111112e-05,
      "loss": 0.219,
      "step": 5600
    },
    {
      "epoch": 0.76,
      "grad_norm": 5.205632209777832,
      "learning_rate": 1.4934222222222223e-05,
      "loss": 0.2206,
      "step": 5700
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 1.230516791343689,
      "learning_rate": 1.4845333333333333e-05,
      "loss": 0.1689,
      "step": 5800
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 3.770902156829834,
      "learning_rate": 1.4756444444444447e-05,
      "loss": 0.1723,
      "step": 5900
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.462549686431885,
      "learning_rate": 1.4667555555555558e-05,
      "loss": 0.2192,
      "step": 6000
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 10.055871963500977,
      "learning_rate": 1.4578666666666668e-05,
      "loss": 0.2056,
      "step": 6100
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 1.1419938802719116,
      "learning_rate": 1.4489777777777779e-05,
      "loss": 0.1927,
      "step": 6200
    },
    {
      "epoch": 0.84,
      "grad_norm": 2.505413293838501,
      "learning_rate": 1.440088888888889e-05,
      "loss": 0.2134,
      "step": 6300
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 7.1111884117126465,
      "learning_rate": 1.4312000000000001e-05,
      "loss": 0.19,
      "step": 6400
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 3.2882981300354004,
      "learning_rate": 1.4223111111111112e-05,
      "loss": 0.2012,
      "step": 6500
    },
    {
      "epoch": 0.88,
      "grad_norm": 15.35948371887207,
      "learning_rate": 1.4134222222222222e-05,
      "loss": 0.1934,
      "step": 6600
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 9.994057655334473,
      "learning_rate": 1.4045333333333336e-05,
      "loss": 0.1857,
      "step": 6700
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 5.342377185821533,
      "learning_rate": 1.3956444444444446e-05,
      "loss": 0.198,
      "step": 6800
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.39314156770706177,
      "learning_rate": 1.3867555555555557e-05,
      "loss": 0.1873,
      "step": 6900
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 5.114381313323975,
      "learning_rate": 1.3778666666666667e-05,
      "loss": 0.1634,
      "step": 7000
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.2909447252750397,
      "learning_rate": 1.368977777777778e-05,
      "loss": 0.1987,
      "step": 7100
    },
    {
      "epoch": 0.96,
      "grad_norm": 3.612912178039551,
      "learning_rate": 1.360088888888889e-05,
      "loss": 0.1789,
      "step": 7200
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 4.32391881942749,
      "learning_rate": 1.3512e-05,
      "loss": 0.2158,
      "step": 7300
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 8.588090896606445,
      "learning_rate": 1.3423111111111111e-05,
      "loss": 0.2101,
      "step": 7400
    },
    {
      "epoch": 1.0,
      "grad_norm": 9.23489761352539,
      "learning_rate": 1.3334222222222225e-05,
      "loss": 0.1979,
      "step": 7500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.940921052631579,
      "eval_f1": 0.940907961499241,
      "eval_loss": 0.1814747452735901,
      "eval_precision": 0.9409919119802103,
      "eval_recall": 0.940921052631579,
      "eval_runtime": 7.8409,
      "eval_samples_per_second": 969.278,
      "eval_steps_per_second": 60.58,
      "step": 7500
    }
  ],
  "logging_steps": 100,
  "max_steps": 22500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3018199322070912.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
